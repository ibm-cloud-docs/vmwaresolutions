---

copyright:

  years:  2016, 2019

lastupdated: "2019-02-16"

---

# 네트워크 확장 및 가상 머신 마이그레이션
{: #vcshcx-stretching}

## 네트워크 확장
{: #vcshcx-network-stretching}

### 네트워크 확장의 개념 및 우수 사례
{: #vcshcx-stretching-best-practices-network}

클라이언트 측 네트워크를 클라우드 측 VXLAN에 연결하는 접점은 전용 HCX 기술로 구성된 정교한 다중 터널 VPN입니다. NSX를 기반으로 하지 않지만 NSX와 함께 작동하며 기능을 확장합니다. 이 프로세스는 클라이언트 측 vCenter 웹 사용자 인터페이스(UI)를 통해 제어하며 배포를 자동화하고 클라이언트와 클라우드 측의 두 엔드포인트를 모두 표시하는 데 사용합니다. 확장할 네트워크는 개별적 또는 일괄처리를 통해 선택합니다.

또한 네트워크 확장 워크플로우의 일부로, 클라우드 측의 NSX에 VXLAN을 빌드하도록 지시하면 VXLAN이 지정된 클라우드 측 L3 디바이스(연결되지 않은 상태로 남아 있는 DLR 또는 ESG) 및 클라우드 측 L2C 어플라이언스에 작성된 인터페이스에 연결됩니다.

특정 애플리케이션을 마이그레이션할 때 적용 가능한 가상 머신(VM)에서 사용 중인 모든 네트워크는 일반적으로 {{site.data.keyword.cloud}} 인스턴스로 확장되어야 합니다.

왜 항상 확장되지 않고 일반적으로 확장되는 것입니까? VM을 마이그레이션한 후 클라이언트 측에서 특정 트래픽의 연결을 끊으면 유리할 수 있습니다. 예를 들어 클라우드로 이동할 때 대역폭 사용을 높일 수 있는 VM 게스트 백업 클라이언트가 있습니다. 게스트 내 백업 클라이언트는 클라우드 측의 최신 블록 레벨 백업에서 자동으로 수집되므로 VM을 마이그레이션할 때 필요하지 않습니다.

각 VM에 연결하여 게스트 내 클라이언트 백업 스케줄을 종료해야 하지만, 그 대신 클라이언트의 백업 네트워크 어댑터에 연결하지 않으면(백업 네트워크를 사용하는 경우) 백업에 실패하게 됩니다. 이 상황은 모든 VM이 사후 마이그레이션에 도달하여 게스트 내 백업 클라이언트를 사용 안함으로 설정할 수 있을 때까지 임시로 발생합니다.

단일 L2C의 대역폭은 이론적으로 4Gbps이지만, 단일 L2C 쌍에 있는 모든 확장 네트워크의 한계일 수 있으며 단일 확장 네트워크에서 달성할 수 없습니다. 기반 대역폭이 충분히 할당되어 있으며 대기 시간이 낮은 경우(<~10ms) 단일 확장 네트워크는 ~1Gbps를 달성할 수 있습니다.

### 네트워크를 확장하는 프로세스
{: #vcshcx-stretching-process-stretch}

HCX를 사용하여 네트워크(VLAN 또는 VXLAN)를 확장하려면 클라이언트 측 vCenter 웹 UI에서 다음 단계를 완료하십시오.

1. 포트 그룹을 개별적으로 선택하려면 vCenter 웹 UI에서 **네트워크** 탭으로 이동하여 확장할 네트워크를 마우스 오른쪽 단추로 클릭하고 **하이브리드 조치-> 클라우드로 네트워크 확장**을 선택하십시오.
2. 연결할 클라우드 측 L3 디바이스를 선택하고 사용할 L2C 어플라이언스를 선택하십시오. 현재 기본 게이트웨이와 서브넷 마스크를 CIDR 형식으로 입력하십시오.
3. 화면의 맨 아래에서 **확장**을 클릭하여 네트워크 확장 워크플로우를 시작하십시오.

네트워크 진행상태는 vCenter 클라이언트 태스크 분할창에서 모니터링합니다.

### 근접 라우팅 옵션
{: #vcshcx-stretching-prox-routing}

어떠한 유형의 라우트 최적화도 없이 확장된 네트워크는 모든 L3 액세스를 위해 클라이언트 측으로 다시 라우팅됩니다. 이 Trombone-ing에서는 소스와 대상 VM이 모두 클라우드에 있는 경우에도 패킷이 클라이언트(소스)와 클라우드 사이를 왔다갔다 이동해야 하므로 트래픽 패턴이 비효율적이게 됩니다. HCX의 근접 라우팅 기능은 이 문제와 트래픽의 로컬 유출을 해결하도록 설계되었습니다.

##  vMotion
{: #vcshcx-stretching-vmotion}

HCX의 vMotion 기능은 여러 다른 버전의 vSphere, 개별 SSO 도메인 및 인터넷에서 다양한 유형의 네트워크 연결과 작동하도록 vSphere vMotion 기능을 확장합니다. HCX에서는 연결하는 데 사용하는 네트워크가 안전하지 않다고 간주하므로 연결 유형과 상관없이 항상 암호화된 터널을 통해 트래픽을 이동합니다.

### vMotion의 개념 및 우수 사례
{: #vcshcx-stretching-best-practices-vmotion}

HCX는 본질적으로 vMotion 쌍방향 프록시입니다. HCX의 각 인스턴스에서는 자체적으로 클라우드 게이트웨이 Fleet 컴포넌트(CGW)의 "대행(front)"인 클러스터 외부의 vSphere 데이터 센터에 있는 단일 ESXi 호스트를 에뮬레이트합니다. 프록시 호스트는 현재 표시되는 사이트에 링크된 HCX 사이트마다 표시됩니다. vMotion이 원격 호스트에서 시작되면 로컬 ESXi 호스트가 해당 VM을 로컬 프록시 ESXi 호스트로 vMotion합니다. 이 로컬 프록시 ESXi 호스트는 원격 측에서 CGW와 함께 암호화된 터널을 유지보수하는 CGW를 대행합니다.

동시에, vMotion은 터널 전체의 소스 CGW에서 데이터를 받으므로 원격 ESXi 프록시 호스트에서 대상 vSphere 실제 ESXi 호스트로 시작됩니다. vMotion이 사용되는 경우 대량 마이그레이션 옵션과 달리 한 번에 하나의 VM 마이그레이션 조작만 실행됩니다. 따라서 다량의 VM을 마이그레이션하는 경우 중단 시간을 갖을 수 없거나 VM을 다시 부팅하는 것이 위험한 경우에만 사용하는 것이 좋습니다. 그러나 표준 vMotion과 같이 VM은 프로세스 중에 라이브일 수 있습니다.

단일 vMotion이 LAN에서는 약 ~1.7Gbps를 유지하고 WAN에서는 WAN 최적화 프로그램을 통해 300 ~ 400Mbps를 유지하는 것으로 관찰되었습니다. 그렇다고 해서 LAN의 1.7Gbps가 LAN의 400Mbps와 같다는 의미는 아니며 환경당 관찰된 최대값이 유지된다는 것입니다. 이 값이 관찰된 환경은 10GB LAN vMotion 네트워크, 1GB 인터넷 업링크로 구성되며, 이는 프로덕션 웹 트래픽과 공유합니다.

다음과 같은 경우 vMotion을 사용하십시오.
- VM을 종료 또는 시작하기가 어렵거나 가동 시간이 길고 VM을 종료하면 위험이 발생합니다.
- Oracle Rac 클러스터와 같이 디스크 UUID가 필요한 클러스터 유형 애플리케이션이 있습니다. vMotion에서 대상의 디스크 UUID를 변경하지 않습니다.
- 단일 VM을 가능한 빨리 이동하려고 합니다.
- 스케줄된 마이그레이션이 필요하지 않습니다.

### 조작
{: #vcshcx-stretching-operation}

HCX 웹 UI 스냅인 포털 또는 vSphere 웹 클라이언트 컨텍스트 확장 메뉴를 사용하여 교차 클라우드 vMotion을 시작하십시오. 두 경우 모두 동일한 마이그레이션 마법사가 표시됩니다. 컨텍스트 메뉴의 경우 마이그레이션 조작에 사용하도록 단일 VM만 선택합니다. 포털의 경우 여러 VM을 선택할 수 있습니다.

역마이그레이션 VM은 HCX 마이그레이션 마법사에서 **역마이그레이션** 선택란을 사용하는 웹 UI 포털에서만 가능합니다.

## 대량 마이그레이션
{: #vcshcx-stretching-bulk-mig}

### 대량 마이그레이션의 개념 및 우수 사례
{: #vcshcx-stretching-best-practices-bulk-mig}

HCX의 대량 마이그레이션 기능에서는 vSphere 복제를 사용하여 디스크 데이터를 마이그레이션하면서 대상 vSphere HCX 인스턴스에서 VM을 다시 작성합니다. VM을 마이그레이션하면 다음 워크플로우가 발생합니다.
- 대상 측과 해당 가상 디스크에 새 VM을 작성합니다.
- VM 데이터를 새 VM으로 복제합니다. 복제는 전환 스케줄링과 상관없이 마법사가 완료되는 즉시 시작합니다.
- 원래 VM의 전원을 끕니다.
- 전원 끄기 기간 동안 변경 데이터의 최종 복제가 수행됩니다.
- 대상 측에서 새 VM의 전원을 켭니다.
- 클라우드 폴더로 이동한 원래 VM의 이름을 바꾸고 이동합니다.

다음은 vMotion을 통한 대량 마이그레이션의 장점입니다.
- 여러 VM을 동시에 마이그레이션합니다.
- 더 일관된 대역폭을 사용합니다. vMotion에서는 네트워크 모니터링 도구와 WAN Opt UI에서 최대와 최소로 표시되는 대역폭 사용의 변동을 생성할 수 있습니다.
- 대량 마이그레이션을 사용하면 단일 vMotion에서 가능한 것보다 높은 전체 네트워크 대역폭 기능을 사용할 수 있습니다.
- 스케줄된 가동 중단 기간 동안 새로 마이그레이션된 VM으로 반전하도록 대량 마이그레이션을 스케줄링합니다.
- vMotion이 실패하는 클라우드 측과 다른 가상 CPU 기능을 현재 사용 중인 VM을 마이그레이션할 수 있습니다.

다음은 vMotion을 통한 대량 마이그레이션의 단점입니다.
- 개별 VM은 vMotion 보다 훨씬 느리게 마이그레이션됩니다.
- 새로운 복제 VVM이 대상 측에 표시되므로 VM이 잠시 가동 중단됩니다.
- 주문 순서 지정 및 디스크 UUID(Oracle RAC)에 따라 달라지는 VVM에 문제가 있을 수 있고 UUID가 변경됨에 따라 디스크가 다르게 표시될 수 있습니다. 따라서 가상 디스크 디바이스의 OS 경로가 변경될 수 있습니다.

## 마이그레이션 유형 우수 사례
{: #vcshcx-stretching-mig-type-best-practices}

### 공유 디스크 클러스터
{: #vcshcx-stretching-shared-disk-clusters}

Oracle RAC, MS Exchange 및 MS-SQL 클러스터는 모든 VM 또는 클러스터 노드 간에 디스크를 공유해야 하는 클러스터에 두 개 이상의 VM이 참여하는 애플리케이션의 예입니다. 애플리케이션 클러스터의 일부인 디스크(비OS 가상 디스크)용 모든 VM 노드에서 VMware 다중 기록기 플래그를 사용으로 설정해야 합니다. 가상 디스크에 다중 기록기 플래그가 사용된 VM은 지원되지 않습니다.

다중 기록기 가상 디스크가 사용된 클러스터 마이그레이션:
- vMotion을 원래 VM 디스크로 사용하고 UUID 맵핑이 유지보수됩니다.
- 마이그레이션 중에 클러스터는 성능 저하 상태(단일 노드)로 남아 있습니다.
- 클러스터 VM 노드 전체에서 다중 기록기 구성을 리어셈블하기 위해 마이그레이션 전 및 마이그레이션이 완료된 후 클러스터가 작동 중단됩니다.

다중 기록기 디스크를 사용하는 클러스터를 마이그레이션하려면 다음 단계를 완료하십시오.
1. 애플리케이션 우수 사례별로 모든 노드와 클러스터를 중지합니다.
2. 애플리케이션에 필요한 경우 다중 기록기가 구성된 가상 디스크의 각 노드 VM에서 디스크 순서를 기록합니다.
3. Oracle 및 가상 디스크 UUID 기능을 사용하는 기타 모든 애플리케이션의 경우 특정 ESXi 호스트에 로그인한 후 `vmkfstools -J getuuid /vmfs/volumes/datastore/VM/vm.vmdk` 명령을 실행하여 클러스터의 다중 기록기 플래그를 설정해야 하는 각 가상 디스크 파일의 UUID를
가져옵니다.
  우수 사례에서 디스크 순서를 운영 체제에 경로가 표시되는 방법에 맞게 조정하는 경우 필요합니다. vMotion은 디스크(disk1, disk2, disk3)를 다시 정렬할 수 있지만 UUID는 그대로 남아 있습니다.
  디스크 맵핑 정보에 명시된 UUID를 사용하여 디스크 이름 지정 순서를 다시 작성하십시오. 필요한 경우 마이그레이션이 완료되면 Scsi ID를 사용하십시오. 어떤 경우에든 애플리케이션이 작동해야 합니다. Oracle 인스턴스에 애플리케이션의 문제점 해결을 위해 여러 가상 디스크가 맵핑된 경우 사용합니다.
4. 기본으로 간주되는 가상 디스크를 제외하고 모든 클러스터 VM 노드에서 가상 디스크를 제거합니다.
5. 현재 유일하게 클러스터 디스크를 소유해야 하는 기본 VM 클러스터 노드에서 다중 기록기 플래그를 제거합니다.
6. 최소 가동 중단에 필요한 경우 기본 클러스터 노드를 다시 표시합니다.
7. vMotion으로 모든 클러스터 노드를 마이그레이션합니다. 전원이 꺼져 있을 때 기본 클러스터를 먼저 마이그레이션하고 기타 모든 노드를 마이그레이션합니다.
8. 기본 디스크 소유 노드에서 마이그레이션을 완료하면 전원을 끕니다.
9. 필요한 경우 적절한 디스크 UUID와 SCSI ID를 사용하여 디스크 순서를 다시 맵핑합니다. 애플리케이션이 작동하는 데 재맵핑은 필요하지 않습니다.
10. 기본 노드에서 다중 기록기 플래그를 다시 사용하도록 설정합니다.
11. 기본 노드를 시작하고 조작을 확인합니다.
12. 기타 모든 클러스터 노드 VM에서 다중 기록기 플래그를 사용으로 설정 / 디스크를 맵핑하고 전원을 켭니다
13. 다른 클러스터 노드 조작을 확인하십시오.

### 일반 VM
{: #vcshcx-stretching-general-vms}

HCX의 기능을 중심으로 신뢰도를 빌드하고 나면 대량 마이그레이션을 사용해야 합니다. 중복 애플리케이션에 관한 경우 대량 마이그레이션이 필요합니다. 예를 들어 웹 서버와 수백 개 또는 수천 개의 VM을 마이그레이션하는 경우가 해당됩니다.

### 직접 접속 NAS를 사용하는 VM
{: #vcshcx-stretching-vms-direct-nas}

NFS는 일반적으로 웹 서버 컨텐츠와 같이 여러 서버에서 데이터를 공유하는 데 사용하기 위해 채택합니다. iSCSI는 이메일이나 RDBMS와 같은 애플리케이션 클러스터로 구성되는 VM 노드 간에 사용할 수 있으며 일반적으로 NFS보다 대기 시간의 영향을 많이 받습니다.

어떤 경우에든 {{site.data.keyword.CloudDataCent_notm}}의 대기 시간이 낮게 유지되고(iSCSI의 경우 < ~7ms 및 애플리케이션에서 NFS에 허용하는 시간) 해당 애플리케이션에서 ~1Gbps 이하의 대역폭으로 작동할 수 있으면 NAS 네트워크가 HCX와 함께 {{site.data.keyword.cloud_notm}} 위치로 확장될 수 있습니다. 이 작업을 완료하고 나면 HCX와 함께 정상적으로 VM을 마이그레이션 / VMotion을 수행할 수 있습니다.

마이그레이션 후에 iSCSI 볼륨은 OS와 함께 다른 로컬 클라우드 스토리지 솔루션으로 미러링될 수 있고 NFS 데이터는 클라우드 솔루션의 데이터로 복제될 수 있습니다. 고려해야 하는 사항은 다음과 같습니다.
- 대기 시간(iSCSI 또는 NFS의 애플리케이션 허용 범위)
- 대역폭(확장된 네트워크당 ~1Gbps)
- 기본 링크 대역폭

마이그레이션 라이프사이클 다음에 프로덕션을 시도하기 전에 개발 또는 스테이징 애플리케이션을 테스트하십시오. QoS는 대기 시간의 영향을 많이 받는 확장 L2 네트워크를 수반하는 L2C HCX 어플라이언스 사이의 기본 터널 트래픽(udp 500 / 4500)에 사용할 수 있습니다.

## 네트워크 전환(Swing)
{: #vcshcx-stretching-network-swing}

데이터 센터를 모두 {{site.data.keyword.cloud_notm}}로 내보내려는 경우 HCX 제거 전에 마지막에서 두 번째로 수행할 단계는 네트워크 전환(Swing)입니다. 네트워크 전환에서는 소스 데이터 센터에서 {{site.data.keyword.cloud_notm}}의 NSX 오버레이 네트워크로 마이그레이션된 VM을 저장하는 네트워크 서브넷의 마이그레이션을 수행합니다.

네트워크 전환에서는 다음을 수행합니다.
- 네트워크에서 모든 워크로드를 내보내고 비VM 네트워크 디바이스가 다른 네트워크로 이동되었거나 기능적으로 클라우드에 마이그레이션되었거나 더 이상 사용되지 않는지 확인합니다.
- 네트워크 전환을 지원하기 위해 NSX 토폴로지 또는 {{site.data.keyword.cloud_notm}} 지원 네트워크 토폴로지가 완료되었는지 확인합니다. 예를 들어 동적 라우팅 프로토콜과 방화벽이 있습니다.
- UI에서 HCX 확장 해제 네트워크 플로우를 실행하고 확장 해제 네트워크 기본 게이트웨이를 인계할 적절한 라우팅 NSX 디바이스를 선택합니다.
- 다음과 같은 외부 라우팅 변경을 실행합니다. 마이그레이션된 네트워크의 변경된 라우팅을 삽입하고, 소스 사이트에서 마이그레이션된 네트워크로의 라우트를 제거하고, 필요한 경우 마이그레이션되지 않은 애플리케이션용으로 WAN 전체에 마이그레이션된 서브넷으로 라우팅하게 합니다. 
- 애플리케이션 소유자가 가능한 모든 액세스 지점(인터넷, 인트라넷 및 VPN)에서 마이그레이션 애플리케이션을 테스트합니다.

모든 VM이 완전히 클라우드로 마이그레이션된 특정 애플리케이션에서 네트워크 전환을 수행하기 위한 고려사항은 다음과 같습니다.
사설 네트워크 측에서 vyatta를 사용하여 MPLS 클라우드에 라우트를 삽입하고 {{site.data.keyword.cloud_notm}} IP 공간을 방지하기 위해 MPLS의 에지 라우팅 디바이스로 터널링하는 경우 {{site.data.keyword.cloud_notm}} VRF로 설정된 계정이 있습니다. 일부 애플리케이션은 네트워크 로드 밸런싱 가상 IP(vIP) 뒤에 있습니다. 해당 vIP는 vyatta 뒤의 가상 F5에 있는 소유 서브넷에 있습니다. HCX를 통해 {{site.data.keyword.cloud_notm}}로 전환된 네트워크를 위해 MPLS에 더 구체적인 라우팅을 광고하면 다른 네트워크에서는 더 잘 작동하는 반면 개별 vIP에는 작동하지 않습니다. /32 라우트가 삽입 중이기 때문입니다.

솔루션: WAN 제공자가 광고하는 /32 라우트를 필터링하여 제외하는 것이 일반적입니다. WAN 공급업체와 작업하여 허용하십시오.

고려사항 및 그 영향은 다음과 같습니다.
- 서브넷, vLAN 및 VXLAN을 공유하는 애플리케이션은 함께 이동해야 합니다.
- 내부 라우트 가능 IP를 사용하는 로드 밸런서 뒤에 있는 애플리케이션의 경우 함께 이동할 수 없거나 함께 이동하는 것이 바람직하지 않은 경우 라우팅을 변경해야 할 수도 있습니다. 예를 들어 한 번에 너무 많은 애플리케이션을 전환하므로 위험이 너무 많이 인지되는 경우가 해당됩니다.
- VMware 관리자, 네트워크 관리자(고객/WAN 공급업체 포함), 애플리케이션 소유자가 관련되어야 합니다. 변경이 계획되지 않은 경우에도 해당 변경사항이 특정 시스템이나 네트워크 장비에 영향을 미칩니다.

## 관련 링크
{: #vcshcx-stretching-related}

* [vCenter Server on {{site.data.keyword.cloud_notm}} with Hybridity Bundle 개요](/docs/services/vmwaresolutions/archiref/vcs/vcs-hybridity-intro.html)   
