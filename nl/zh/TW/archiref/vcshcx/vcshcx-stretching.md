---

copyright:

  years:  2016, 2019

lastupdated: "2019-02-16"

---

# 網路延伸和虛擬機器移轉
{: #vcshcx-stretching}

## 網路延伸
{: #vcshcx-network-stretching}

### 網路延伸的概念及最佳作法
{: #vcshcx-stretching-best-practices-network}

橋接用戶端網路至雲端 VXLAN 的中介，是一個複雜的多通道 VPN，其包含專利的 HCX 技術。它不是基於 NSX，但與 NSX 搭配運作並延伸其功能。此處理程序是由用戶端 vCenter Web 使用者介面 (UI) 控制，它會自動部署，並同時啟動用戶端和雲端的兩個端點。選取延伸的網路可個別執行或批次處理。

此外，在網路延伸工作流程中，雲端上的 NSX 被指示建置 VXLAN，其後將連接至所指定雲端 L3 裝置上建立的介面（DLR 或 ESG 處於未連接狀態），以及雲端 L2C 應用裝置。

當您移轉特定應用程式時，適用的虛擬機器 (VM) 正在使用的所有網路，一般都必須延伸到 {{site.data.keyword.cloud}} 實例。

為什麼是一般而不是一律這樣？在 VM 移轉之後，從用戶端中斷特定資料流量會有幫助。例如，VM 來賓備份用戶端，這可能導致移至雲端時的高頻寬使用率。當移轉 VM 時，並不需要來賓備份用戶端，因為在雲端上會有更現代的區塊層次備份自動挑選它。

非存取每個 VM 來關閉來賓用戶端備份排程，而是未連接用戶端的備份網路配接卡（如果已使用備份網路）導致備份失敗。這是暫時狀況，直到移轉後能呼叫到所有 VM 來停用來賓備份用戶端，此狀況才會結束。

單一 L2C 的頻寬理論上為 4 Gbps，不過，這對於單一 L2C 配對內的所有延伸網路會是限制，且單一延伸網路無法達到此頻寬。假設有分配到足夠的支撐頻寬，且延遲很低（<~10 毫秒），則單一延伸網路可以達到 ~1 Gbps。

### 延伸網路的處理程序
{: #vcshcx-stretching-process-stretch}

若要以 HCX 來延伸網路（VLAN 或 VXLAN），請從用戶端 vCenter Web 使用者介面完成下列步驟。

1. 若要個別選取埠群組，請導覽至 vCenter Web 使用者介面內的**網路**標籤，在要延伸的網路上按一下滑鼠右鍵，然後選取**Hybridity 動作-> 延伸網路至雲端**。
2. 選取要連接的雲端 L3 裝置，以及將要使用的 L2C 應用裝置。輸入 CIDR 格式的現行預設閘道及子網路遮罩。
3. 按一下畫面底端的**延伸**，以開始網路延伸工作流程。

在 vCenter 用戶端作業窗格中監視網路進度。

### 近似性遞送選項
{: #vcshcx-stretching-prox-routing}

如果沒有任何類型的遞送最佳化，則延伸網路遞送回到用戶端以存取任何 L3。這樣的拉長作用會產生沒有效率的資料流量模式，因為封包需要在用戶端（來源）與雲端之間來回傳送，即使來源與目的地 VM 都在雲端內的情況下也是如此。HCX 的「近似性遞送」特性是設計來處理這項以及資料流量的本端出口。

##  vMotion 
{: #vcshcx-stretching-vmotion}

HCX 內的 vMotion 功能可延伸 vSphere vMotion 功能，能在不同版本的 vSphere、個別的 SSO 網域以及網際網路上不同類型的網路連線之間運作。HCX 假設它用來連接的網路並不安全，而且無論連線類型為何，一律都會透過加密通道來移動資料流量。

### vMotion 的概念和最佳作法
{: #vcshcx-stretching-best-practices-vmotion}

HCX 實際上就是 vMotion 雙向 Proxy。每一個 HCX 實例模擬 vSphere 資料中心內的單一 ESXi 主機，在任何叢集之外，它本身就是雲端閘道機群元件 (CGW) 的「前端」。對於每一個連結至目前所檢視網站的 HCX 網站，都會出現一個 Proxy 主機。當 vMotion 起始至遠端主機時，本端 ESXi 主機會以 vMotion 讓 VM 連接至本端 Proxy ESXi 主機，其成為 CGW 的前端，同時使用遠端系統上的 CGW 來維護加密通道。

同時，會從遠端 ESXi Proxy 主機起始 vMotion 至目的地 vSphere 實體 ESXi 主機，因為它透過通道接收來源 CGW 的資料。當採用 vMotion 時，與大量移轉選項不同，一次只會執行一個 VM 移轉作業。因此，如果要移轉大量 VM，建議僅在關閉時間不是選項，或在重新啟動 VM 有風險時才使用它。不過，就像標準 vMotion 一樣，VM 在此處理程序期間內可以是即時狀態。

我們觀察到，單一 vMotion 在 LAN 上大約 ~1.7 Gbps 以及（透過 WAN 最佳化工具）在 WAN 上 300 到 400 Mbps 將達到最高點。這不表示 LAN 上的 1.7 Gbps 等於 LAN 上的 400 Mbps，而是對每個環境觀察到的最大數目。觀察到的環境包含 10 GB LAN vMotion 網路、1GB 網際網路上行鏈路，後者與正式作業 Web 資料流量共用。

在下列情況下使用 vMotion：
- VM 很難關閉、啟動，或執行時間較長，關閉它會帶來風險。
- 有任何叢集類型應用程式需要磁碟 UUID，例如 Oracle Rac 叢集。vMotion 未變更目的地上的磁碟 UUID。
- 您想要儘快移動單一 VM。
- 不需要排定的移轉。

### 作業
{: #vcshcx-stretching-operation}

使用 HCX Web 使用者介面嵌入式管理入口網站或 vSphere Web 用戶端環境定義延伸功能表，來起始跨雲端 vMotion。在任一情況下，都會啟動相同的移轉精靈。對於環境定義功能表，只會選取單一 VM 進行移轉作業。對於入口網站，可以選取多個 VM。

反轉移轉 VM 只能從 Web 使用者介面入口網站進行，即使用「HCX 移轉」精靈中的**反轉移轉**勾選框。

## 大量移轉
{: #vcshcx-stretching-bulk-mig}

### 大量移轉的概念和最佳作法
{: #vcshcx-stretching-best-practices-bulk-mig}

HCX 的大量移轉功能會使用 vSphere 抄寫來移轉磁碟資料，同時在目的地 vSphere HCX 實例上重建 VM。移轉 VM 需要下列工作流程：
- 在目的地端及其對應的虛擬磁碟上建立新的 VM。
- 將 VM 資料抄寫至新的 VM。無論是否切換排程，只要完成精靈，抄寫就會立即開始。
- 關閉原始 VM 的電源。
- 在關閉期間，會發生任何變更資料的最終抄寫。
- 在目的地端開啟新 VM 的電源。
- 將原始 VM 重新命名並移動至「移至雲端資料夾」。

以下是大量移轉優於 vMotion 的優點：
- 可並行移轉許多 VM。
- 頻寬使用更一致。vMotion 可以產生頻寬使用的變動，這些變動在網路監視工具內或 WAN 最佳化工具使用者介面中會顯示為尖峰及低谷。
- 使用大量移轉，比單一 vMotion 能夠達到網路頻寬功能更高的整體使用率。
- 在排定停電時間範圍內，排定大量移轉來翻轉到新移轉的 VM。
- 可以容許 VM 移轉，其目前使用的虛擬 CPU 特性與 vMotion 失敗的雲端不同。

以下是大量移轉劣於 vMotion 的缺點：
- 個別 VM 移轉的速度比 vMotion 慢很多。
- 在目的地端啟動新的複製 VM 時，VM 需要暫時關閉。
- 取決於磁碟排序及磁碟 UUID (Oracle RAC) 的 VM 可能會發生問題，其磁碟顯示會因為 UUID 變更而不同，這可能會將 OS 路徑變更至虛擬磁碟裝置。

## 移轉類型最佳作法
{: #vcshcx-stretching-mig-type-best-practices}

### 共用磁碟叢集
{: #vcshcx-stretching-shared-disk-clusters}

Oracle RAC、MS Exchange 和 MS-SQL 叢集是應用程式範例，其中兩個以上的 VM 參與叢集，且需要在所有 VM 或叢集節點上共用磁碟。在屬於應用程式叢集的磁碟（非 OS 虛擬磁碟）的所有 VM 節點上，需要啟用 VMware 多寫入器旗標。不支援對任何虛擬磁碟啟用多寫入器旗標的 VM。

移轉已啟用多寫入器虛擬磁碟的叢集：
- 會使用 vMotion 作為原始 VM 磁碟，並維護 UUID 對映。
- 在移轉期間，叢集在欠佳狀態下維持啟動（單一節點）。
- 在移轉開始之前及移轉完成之後叢集需要關閉，以便在叢集 VM 節點之間重新組合多寫入器配置。

完成下列步驟，以移轉啟用多寫入器磁碟的叢集：
1. 根據應用程式最佳作法，卸下叢集和所有節點。
2. 如果應用程式有需要，請記下每個節點 VM 中針對多寫入器配置的虛擬磁碟的磁碟順序。
3. 若為 Oracle 和使用虛擬磁碟 UUID 特性的其他任何應用程式，請登入特定 ESXi 主機，並執行 `vmkfstools -J getuuid /vmfs/volumes/datastore/VM/vm.vmdk` 指令，以取得需要為該叢集設定多寫入器旗標的每個虛擬磁碟檔的 UUID。如果最佳作法是使磁碟順序與路徑在作業系統中的顯示方式一致，則這是必要的。vMotion 可以將磁碟重新排序（disk1、disk2、disk3），但 UUID 維持相同。請利用指定的 UUID 來產生磁碟對映資訊，以重建磁碟命名順序。必要的話，移轉完成時使用 Scsi ID。不論哪一種方式，應用程式應該都能運作。當 Oracle 實例具有許多虛擬磁碟是為了應用程式疑難排解而對映時，會使用此選項。
4. 從所有叢集 VM 節點中移除虛擬磁碟（但被認為是主要的節點除外）。
5. 從主要 VM 叢集節點中移除多寫入器旗標，該節點應該是目前唯一擁有叢集磁碟的節點。
6. 視需要，將主要叢集節點啟動，使關閉時間縮至最少。
7. 使用 vMotion 移轉所有叢集節點。先移轉主要叢集，所有其他節點在關閉電源時移轉。
8. 當主要磁碟擁有節點完成移轉時，關閉電源。
9. 必要的話，以適當的磁碟 UUID 和 Scsi ID 重新對映磁碟順序。應用程式要能運作，並不需要重新對映。
10. 在主要節點上重新啟用多寫入器旗標。
11. 啟動主要節點並驗證作業。
12. 在所有其他叢集節點 VM 上對映磁碟 / 啟用多寫入器旗標，然後開啟電源。
13. 驗證其他叢集節點作業。

### 一般 VM
{: #vcshcx-stretching-general-vms}

對 HCX 的功能建立信賴度之後，必須採用大量移轉。如果重視備援應用程式，則需要進行大量移轉。例如，Web 伺服器以及有許多 100s 或 1000s VM 要移轉的裝置。

### 使用直接連接 NAS 的 VM
{: #vcshcx-stretching-vms-direct-nas}

NFS 通常用來共用許多伺服器之間的資料，例如 Web 伺服器內容。在包含應用程式叢集（例如電子郵件或 RDBMS）的 VM 節點之間可以採用 iSCSI，它通常比 NFS 更加有延遲敏感性。

在任何一種情況下，如果延遲能夠低到 {{site.data.keyword.CloudDataCent_notm}}（對於 iSCSI 為 < ~7 毫秒，以及應用程式對 NFS 能夠容忍的數字），並容許應用程式以 ~1 Gbps 或更小的頻寬操作，則 NAS 網路能以 HCX 延伸到 {{site.data.keyword.cloud_notm}} 位置。完成這項作業之後，VM 就可以像平常一樣以 HCX 移轉 / vMotion。

在移轉後，iSCSI 磁區可以與 OS 鏡映到另一個本端雲端儲存空間解決方案，且 NFS 資料可以抄寫到任何雲端解決方案。考量如下：
- 延遲（iSCSI 或對 NFS 的應用程式容錯）
- 頻寬（每個延伸網路為 ~1 Gbps）
- 支撐鏈結頻寬

遵循移轉生命週期，確保在嘗試正式作業之前，先使用開發或暫置應用程式來進行測試。您可以在使用延遲敏感延伸 L2 網路的 L2C HCX 應用裝置之間，對支撐通道資料流量 (udp 500 / 4500) 採用 QoS（服務品質）。

## 網路擺盪 (Network Swing)
{: #vcshcx-stretching-network-swing}

如果目標是資料中心撤離至 {{site.data.keyword.cloud_notm}}，則在 HCX 移除之前的倒數第二個步驟是網路擺盪 (network swing)。網路擺盪可達到網路子網路移轉的目的，這些存放著所移轉 VM 的子網路從來源資料中心擺盪到 {{site.data.keyword.cloud_notm}} 內的 NSX 覆蓋網路。

擺盪網路需要執行下列動作：
- 驗證網路的所有工作負載是否已全部退出，且任何非 VM 網路裝置是否移至另一網路，是功能性移轉至雲端或已淘汰。
- 驗證已完成任何 NSX 拓蹼或 {{site.data.keyword.cloud_notm}} 支援的網路拓蹼，以支援網路擺盪。例如，動態遞送通訊協定和防火牆。
- 在使用者介面中執行 HCX 取消延伸網路工作流程，並選取適當的遞送 NSX 裝置來接管延伸網路預設閘道。
- 執行任何外部遞送變更（包括：插入已移轉之網路的已變更遞送），移除已移轉網路的來源網站的遞送，必要的話，確保未移轉的應用程式透過 WAN 遞送至已移轉子網路。
- 應用程式擁有者從所有可能的存取點測試已移轉的應用程式：網際網路、企業內部網路和 VPN。

考慮對於所有 VM 都已完全移轉至雲端的特定應用程式執行網路擺盪：如果您是在專用網路端使用 vyatta 將遞送插入至 MPLS 雲端，並透過通道到達 MPLS 上的邊緣遞送裝置，以避免 {{site.data.keyword.cloud_notm}} IP 空間。您已使用 {{site.data.keyword.cloud_notm}} VRF 設定您的帳戶。部分應用程式位於網路負載平衡虛擬 IP (vIP) 背後。那些 vIP 位於您自己的子網路上，而這個子網路位於 vyatta 背後的虛擬 F5 上。雖然透過 HCX 向擺盪到 {{site.data.keyword.cloud_notm}} 的網路公告更具體遞送至 MPLS 對其他網路很有用，但對於個別 vIP 沒有用，因為其插入 /32 遞送。

解決方案：通常是由 WAN 提供者濾出所公告的 /32 遞送。請與 WAN 供應商洽談是否能這麼做。

以下是注意事項及含意：
- 共用子網路、vLAN 及 VXLAN 的應用程式必須一起移動。
- 使用內部可遞送 IP 的負載平衡器背後的應用程式可以要求遞送變更（如果它們無法一起移動或不想要這樣做的話）。例如，因為一次擺盪涉及太多應用程式而發現風險太高。
- 需要涉及 VMware 管理者、網路管理者（包括客戶 / WAN 供應商）、應用程式擁有者。即使不在規劃中，這些變更仍會影響特定系統或網路設備。

## 相關鏈結
{: #vcshcx-stretching-related}

* [vCenter Server on {{site.data.keyword.cloud_notm}} with Hybridity Bundle 概觀](/docs/services/vmwaresolutions/archiref/vcs?topic=vmware-solutions-vcs-hybridity-intro)   
