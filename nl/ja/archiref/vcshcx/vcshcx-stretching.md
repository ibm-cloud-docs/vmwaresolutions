---

copyright:

  years:  2016, 2019

lastupdated: "2019-03-05"

subcollection: vmware-solutions


---

# ネットワーク・ストレッチと仮想マシンのマイグレーション
{: #vcshcx-stretching}

## ネットワーク・ストレッチ
{: #vcshcx-network-stretching}

### ネットワーク・ストレッチの概念とベスト・プラクティス
{: #vcshcx-stretching-best-practices-network}

クライアント・サイドのネットワークをクラウド・サイドの VXLAN に橋渡しするためのかぎとなるのは、独自の HCX テクノロジーで構成される高度なマルチトンネル VPN です。 これは NSX を基礎とするものではありませんが、NSX と連携し、その機能を拡張します。 このプロセスは、クライアント・サイドの vCenter Web ユーザー・インターフェース (UI) によって制御され、デプロイメントを自動化し、クライアントとクラウドの両サイドのエンドポイントを確立します。 ストレッチ対象のネットワークの選択は、個別または一括で行います。

さらに、ネットワーク・ストレッチ・ワークフローの一環として、クラウド・サイドの NSX が権限を取得して VXLAN を構築します。その後 VXLAN が、クラウド・サイドの指定された L3 デバイス (未接続状態のままの DLR または ESG) およびクラウド・サイドの L2C アプライアンスで作成されたインターフェースに接続します。

通常、アプリケーションをマイグレーションする場合は、対象の仮想マシン (VM) によって使用されているすべてのネットワークを {{site.data.keyword.cloud}} インスタンスにストレッチする必要があります。

上の文で「常に」ではなく「通常」と書かれているのはなぜでしょうか? VM のマイグレーション後に特定のトラフィックをクライアント・サイドから切断すると役立つことがあります。 例えば、VM ゲスト・バックアップ・クライアントの場合、クラウドに移動するときに高帯域幅を使用することがあります。 この VM がマイグレーションされるときにゲスト内バックアップ・クライアントは不要です。クラウド・サイドの最新のブロック・レベル・バックアップによって自動選択されるためです。

クライアント・バックアップ・ネットワーク・アダプターにはアクセスしません。そのようにすると、各 VM にアクセスしてゲスト内クライアント・バックアップ・スケジュールをシャットオフすることになるからです。 したがって、バックアップ・ネットワークを使用すると、バックアップが失敗することがあります。 これは、すべての VM がマイグレーション後の処理に到達してゲスト内バックアップ・クライアントを無効化できるようになるまでの一時的な状態です。

単一の L2C の帯域幅は理論上 4 Gbps ですが、これは 1 組の L2C ペア内に設定されたすべてのストレッチ・ネットワークを合わせた限度となるため、単一のストレッチ・ネットワークではその数値に到達不可能です。 アンダーレーの帯域幅が十分に割り振られていて、待ち時間が短い (およそ 10 ミリ秒以下) 場合には、単一のストレッチ・ネットワークで約 1 Gbps を実現可能です。

### ネットワークをストレッチするためのプロセス
{: #vcshcx-stretching-process-stretch}

HCX を利用してネットワーク (VLAN または VXLAN) をストレッチするには、クライアント・サイドの vCenter Web UI から以下のステップを実行します。

1. 選択したそれぞれのポート・グループに関して、vCenter Web UI の**「Networks」**タブに移動し、ストレッチするネットワークを右クリックします。その後、**「Hybridity Actions」->「Extend Networks to the Cloud」**と選択します。
2. 接続先のクラウド・サイドの L3 デバイスと、使用される L2C アプライアンスを選択します。 現在のデフォルトのゲートウェイとサブネット・マスクを CIDR 形式で入力します。
3. 画面下部にある**「Stretch」**をクリックし、ネットワーク・ストレッチ・ワークフローを開始します。

ネットワークの進行状況のモニターは、vCenter Client の「Tasks」ペインで行います。

### 隣接ルーティング・オプション
{: #vcshcx-stretching-prox-routing}

どのタイプの経路最適化機能も使用されていない場合、あらゆる L3 アクセスにおいて拡張ネットワークはクライアント・サイドに戻るようにルーティングされます。 このトロンボーン現象により、非効率的なトラフィック・パターンが生じます。パケットがクライアント (ソース) とクラウド間で行ったり来たりする必要があるからです (ソースと宛先の両方の VM がクラウド内に存在する場合であっても同様です)。 HCX の隣接ルーティング・フィーチャーは、この問題に対処してトラフィックのローカル出口を設けるように設計されています。

## vMotion
{: #vcshcx-stretching-vmotion}

HCX での vMotion 機能は、vSphere vMotion 機能を拡張して、各種バージョンの vSphere、別個の SSO ドメイン、およびインターネット経由の各種タイプのネットワーク接続にわたって機能するようにします。 HCX では、接続に使用されるネットワークがセキュアではないという前提で機能するので、接続の種類に関係なく、トラフィックの移動は必ず暗号化されたトンネルを介して行われます。

### vMotion の概念とベスト・プラクティス
{: #vcshcx-stretching-best-practices-vmotion}

HCX は、両方向で使用できる vMotion プロキシーであると見なすことができます。 HCX の各インスタンスは vSphere データ・センター内の単一の ESXi ホストをエミュレートします。このホストはすべてのクラスターの外部にあり、クラウド・ゲートウェイ・フリート・コンポーネント (CGW) の「フロント」となります。 プロキシー・ホストは、現在の表示サイトにリンクされている HCX サイトごとに表示されます。 vMotion がリモート・ホストに対して開始されると、ローカル ESXi ホストは、CGW のフロントとなるローカル・プロキシー ESXi ホストへ対象 VM を vMotion で移動します。この CGW は、リモート・サイドの CGW との間の暗号化されたトンネルの保守も行います。

これと同時に、宛先の vSphere 物理 ESXi ホストがトンネルを介してソースの CGW のデータを受信すると、リモートの ESXi プロキシー・ホストからこの vSphere 物理 ESXi ホストへの vMotion マイグレーションが開始されます。 vMotion が採用される場合、一括マイグレーション・オプションとは異なり、一度に実行されるのは 1 つの VM マイグレーション操作のみです。 そのため、大量の VM のマイグレーションでは、ダウン時間を許容できない場合や VM をリブートするとリスクが生じたりする場合にのみ、vMotion を使用することをお勧めします。 ただし、標準の vMotion の場合と同様、VM はプロセス中も稼働できます。

これまでの計測結果として、単一の vMotion では、LAN 上で最高約 1.7 Gbps、WAN オプティマイザーを使用する WAN で 300 から 400 Mbps の速度が計測されています。 これは、LAN 上の 1.7 Gbps と WAN オプティマイザー経由の WAN 上の 400 Mbps が等価だという意味ではなく、特定の環境でそうした最大値が計測されたという意味です。 それは、10 GB LAN vMotion ネットワークと 1GB インターネット・アップリンクで構成されていて実働 Web トラフィックと共有する環境でした。

vMotion は以下の場合に使用します。
- VM のシャットダウンや始動に手間がかかる場合、またはアップタイムに時間がかかる可能性があるためシャットダウンによりリスクが生じる場合。
- ディスク UUID が必要なクラスター・タイプ (Oracle RAC クラスターなど) のアプリケーションの場合。 vMotion を実行しても、宛先でディスク UUID は変更されません。
- 単一の VM をできるだけ短時間で移動する場合。
- スケジュールされたマイグレーションが不要である場合。

### 操作
{: #vcshcx-stretching-operation}

HCX Web UI スナップイン・ポータルまたは vSphere Web Client コンテキスト拡張メニューを使用して、クロスクラウド vMotion を開始します。 どちらの場合も、同じマイグレーション・ウィザードが表示されます。 コンテキスト・メニューの場合、マイグレーション操作用に 1 つの VM のみを選択できます。 ポータルの場合には、複数の VM を選択できます。

VM の逆方向のマイグレーションを行えるのは Web UI ポータルからのみです。その場合、HCX マイグレーション・ウィザードで**「Reverse migration」**チェック・ボックスを使用します。

## 一括マイグレーション
{: #vcshcx-stretching-bulk-mig}

### 一括マイグレーションの概念とベスト・プラクティス
{: #vcshcx-stretching-best-practices-bulk-mig}

HCX の一括マイグレーション機能では vSphere レプリケーションを使用して、宛先の vSphere HCX インスタンスで VM を再作成している間にディスク・データをマイグレーションします。 VM のマイグレーションにより、以下のワークフローが生じます。
- 宛先サイドでの新しい VM とその対応する仮想ディスクの作成。
- 新しい VM への VM データのレプリケーション。 レプリケーションは、切り替えスケジュールに関係なく、ウィザードが完了するとすぐに開始します。
- 元の VM のパワーダウン。
- パワーオフ期間中に発生する、変更データの最終レプリケーション。
- 宛先サイドにおける新しい VM のパワーオン。
- 元の VM を移動先のクラウド・フォルダーに名前変更して移動する処理。

vMotion と比較して一括マイグレーションには以下の利点があります。
- 同時に複数の VM をマイグレーションできる。
- より一貫性のある方法で帯域幅を使用できる。 vMotion の場合、使用する帯域幅に変動が生じ、ネットワーク・モニタリング・ツールや WAN オプティマイザー UI で上下動が観測される可能性があります。
- 単一の vMotion の場合と比較して、一括マイグレーションを使用する場合にはネットワーク帯域幅の全体的使用量が高くなる。
- 一括マイグレーションのスケジュールを設定し、計画停止期間中にマイグレーション後の新しい VM に切り替えることができる。
- クラウド・サイドとは異なる仮想 CPU フィーチャーを使用している VM をマイグレーションできる。 その場合は、vMotion マイグレーションが失敗することもあります。

vMotion と比較して一括マイグレーションには以下の欠点があります。
- vMotion に比べ、個々の VM マイグレーション速度がかなり遅い。
- 宛先サイドに新たに VM が複製されると VM で短時間のダウン時間が生じる。
- ディスクの順序とディスク UUID に依存する VM (Oracle RAC) の場合、UUID が変更されて問題が生じ、元とは異なるディスクが表示される可能性がある。それに伴い、仮想ディスク・デバイスへの OS パスが変更されることがあります。

## マイグレーション・タイプに関するベスト・プラクティス
{: #vcshcx-stretching-mig-type-best-practices}

### 共有ディスク・クラスター
{: #vcshcx-stretching-shared-disk-clusters}

Oracle RAC、MS Exchange、MS-SQL などのアプリケーションのクラスターでは、複数の VM が 1 つのクラスターに参加し、すべての VM またはクラスター・ノードが使用する共有ディスクを必要とします。 アプリケーション・クラスターの一部であるディスク (非 OS 仮想ディスク) に対して、すべての VM ノードで VMware マルチライター・フラグを有効にする必要があります。 あらゆる仮想ディスクに対してマルチライター・フラグが有効になっている VM はサポートされていません。

マルチライター仮想ディスクが有効なクラスターのマイグレーション:
- 元の VM ディスクと UUID マッピングが維持されるため、vMotion を使用します。
- クラスターはマイグレーション中に低下状態 (単一ノード) で稼働状態を維持します。
- クラスターでは、クラスター VM 全体でマルチライター構成を再アセンブルするために、マイグレーションの開始前とマイグレーションの完了後にダウン時間が生じます。

マルチライター・ディスクが有効なクラスターをマイグレーションするには、以下のステップを実行します。
1. アプリケーションのベスト・プラクティスに基づいて、クラスターとすべてのノードの電源を切ります。
2. アプリケーションで必要となる場合には、マルチライター構成が設定されている仮想ディスクの各ノード VM におけるディスクの順序をメモに取ります。
3. 仮想ディスク UUID フィーチャーを使用する Oracle などのアプリケーションの場合、特定の ESXi ホストにログインし、`vmkfstools -J getuuid /vmfs/volumes/datastore/VM/vm.vmdk` コマンドを実行して、
クラスターに設定されているマルチライター・フラグを必要とする仮想ディスク・ファイルごとに UUID を取得します。
  これは、ベスト・プラクティスによって、オペレーティング・システムにおけるパスの表示方法に基づいてディスクの順序を調整する場合に必要となります。 vMotion によってディスク (disk1、disk2、disk3) が再配列される可能性はありますが、UUID は同じままです。
  マイグレーションが完了したら、メモしておいた UUID を使用してディスク情報をマップし、ディスクの命名順序と SCSI ID (必要な場合) を再作成します。 いずれかの方法でアプリケーションが機能するようにします。 この方法は、Oracle インスタンスに、アプリケーションのトラブルシューティング用にマップされる仮想ディスクが多数存在する場合に使用されます。
4. プライマリーと見なされているもの以外の仮想ディスクをすべてのクラスター VM から削除します。
5. クラスター・ディスクを現在所有している唯一のプライマリー・クラスター VM からマルチライター・フラグを削除します。
6. ダウン時間を最小にするために必要な場合はプライマリー・クラスターの電源を入れます。
7. vMotion ですべてのクラスター・ノードをマイグレーションします。 まずプライマリー・クラスターをマイグレーションします。 その電源を切ってから、他のすべてのノードをマイグレーションします。
8. プライマリー・ディスクが所有しているノードのマイグレーションが完了したら、その電源をオフにします。
9. 必要に応じて、適切なディスク UUID と scsi ID でディスクの順序を再マップします。 アプリケーションが機能するために再マップが必須であるわけではありません。
10. プライマリー・ノードでマルチライター・フラグを再び有効にします。
11. プライマリー・ノードを開始し、操作を検証します。
12. 他のすべてのクラスター VM でディスクをマップするかマルチライター・フラグを有効化し、それらをパワーオンします。
13. 他のクラスターの操作を検証します。

### 一般の VM
{: #vcshcx-stretching-general-vms}

HCX の機能が正しく動作していることを確認した後に、一括マイグレーションを導入する必要があります。 冗長アプリケーションが関係している場合には、一括マイグレーションが必要です。 たとえば、Web サーバーがあり、マイグレーションする VM 数が数百、数千と多数ある場合です。

### 直接接続された NAS を使用する VM
{: #vcshcx-stretching-vms-direct-nas}

通常 NFS は、Web サーバー・コンテンツなどのデータを多くのサーバーで共有するときに使用されます。 iSCSI は E メールや RDBMS などのアプリケーション・クラスターを構成する VM ノードで導入可能で、通常は NFS に比べて待ち時間の許容度が厳格です。

どちらの場合であっても、{{site.data.keyword.CloudDataCent_notm}} に対する待ち時間を短くできる場合 (iSCSI の場合にはおよそ 7 ms 以下、NFS の場合にはアプリケーションの許容範囲内)、アプリケーションがおよび 1 Gbps 以下の帯域幅で操作を実行できるなら、NAS ネットワークは HCX を使用して {{site.data.keyword.cloud_notm}} の場所にストレッチできます。 これを行った後、VM を通常の方法で HCX でマイグレーションすることや vMotion を行うことができます。

マイグレーション後に、iSCSI ボリュームを OS と一緒に別のローカル・クラウド・ストレージ・ソリューションにミラーリングできます。また、任意のクラウド・ソリューションに NFS データを複製できます。 考慮する点は以下のとおりです。
- 待ち時間 (iSCSI、または NFS におけるアプリケーション許容範囲)
- 帯域幅 (ストレッチ・ネットワークあたり約 1 Gbps)
- アンダーレー・リンク帯域幅

マイグレーション・ライフサイクルが終わったら、実動環境で試す前に開発アプリケーションやステージング・アプリケーションをテストしてください。 待ち時間に厳格なストレッチ L2 ネットワークをサポートする L2C HCX アプライアンス間のアンダーレー・トンネル・トラフィック (UDP 500 / 4500) に関して QoS を導入できます。

## ネットワーク・スイング
{: #vcshcx-stretching-network-swing}

データ・センターを {{site.data.keyword.cloud_notm}} に移動させることが目標である場合は、HCX を除去する前に行う、最後から 2 番目のステップは、ネットワーク・スイングです。 ネットワーク・スイングによって、マイグレーションされた VM を収めるネットワーク・サブネットを、ソースのデータ・センターから {{site.data.keyword.cloud_notm}} の NSX オーバーレイ・ネットワークにマイグレーションできます。

ネットワークのスイングでは以下の手順を実行します。
- ネットワークからすべてのワークロードが移動されてなくなり、非 VM ネットワーク・デバイスすべてが別のネットワークに移動されたか、機能的にクラウドにマイグレーションされたか、または使用されなくなったことを確認します。
- NSX トポロジーまたは {{site.data.keyword.cloud_notm}} サポート・ネットワーク・トポロジーが完成してネットワーク・スイングをサポートしていることを確認します。 例えば、動的ルーティング・プロトコルやファイアウォールなどです。
- UI で HCX ストレッチ解除ネットワーク・ワークフローを実行し、ストレッチを解除したネットワークのデフォルト・ゲートウェイの制御を引き継ぐ適切なルーティング NSX デバイスを選択します。
- その他の外部ルーティング変更を実行します。例えば、マイグレーションしたネットワークに変更済みのルーティングを挿入したり、マイグレーションしたネットワークからソース・サイトへのルートを除去したり、マイグレーションしていないアプリケーションに関して WAN を介したマイグレーション済みサブネットへのルーティングが機能するかどうかを確認したりします。
- 可能性のあるすべてのアクセス・ポイント (インターネット、イントラネット、VPN) からのマイグレーション済みアプリケーションのアプリケーション所有者テスト。

すべての VM を完全にクラウドにマイグレーションした特定のアプリケーションでネットワーク・スイングを実行するとします。 例えば、次のような場合です。
- プライベート・ネットワーク・サイドで vyatta を使用して MPLS クラウドにルーティングを挿入していて、MPLS でエッジ・ルーティング・デバイスに対するトンネルを設けて {{site.data.keyword.cloud_notm}} IP スペースを回避する場合。
- {{site.data.keyword.cloud_notm}} VRF で設定されたアカウントがある場合。
- 一部のアプリケーションがネットワーク・ロード・バランシング仮想 IP (vIP) の背後にある場合。 こうした vIP は、vyatta の背後の仮想 F5 上のユーザー所有のサブネットにあります。

HCX 経由で {{site.data.keyword.cloud_notm}} にスイングするネットワークに関して MPLS へのより具体的なルーティングを追加すると、他のネットワークでは十分に機能しますが、個々の vIP に関しては /32 ルートが追加されるので機能しません。

解決策: 通常、WAN プロバイダーは、追加された /32 ルートをフィルターにかけて除外します。 WAN ベンダーと連携して、これを行えるようにします。

考慮事項と暗黙的に含まれる事柄について以下に示します。
- サブネット、vLAN、VXLAN を共有するアプリケーションは一緒に移動する必要があります。
- ルーティング可能な内部 IP を使用する、ロード・バランサーの背後にあるアプリケーションは、一緒に移動できない場合や、移動したくない場合にはルーティング変更を要求できます。 例えば、1 度のスイングで関係するアプリケーションが多すぎて、被るリスクが大きすぎる場合です。
- 計画上は特定のシステムやネットワーク機器への影響がない場合でも、VMware 管理者、ネットワーク管理者 (お客様や WAN ベンダーを含む)、アプリケーション所有者の関与が必要です。

## 関連リンク
{: #vcshcx-stretching-related}

* [vCenter Server on {{site.data.keyword.cloud_notm}} with Hybridity Bundle の概要](/docs/services/vmwaresolutions/archiref/vcs?topic=vmware-solutions-vcs-hybridity-intro)   
